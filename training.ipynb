{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5fff39-b102-4357-b758-90684a735796",
   "metadata": {},
   "source": [
    "# AgileAI training notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f10221-ab84-40e6-83fd-bb28d98b1a66",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc204cb-2291-45d3-96b9-1a4d26ed726b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pypdf in /home/deshpande.sh/.local/lib/python3.10/site-packages (6.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from pypdf) (4.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6697425c-4826-4f38-9c20-65b8394a79ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "INPUT_FOLDER = \"dataset\"          # folder where your PDFs are saved\n",
    "OUTPUT_FOLDER = \"clean_text\"      # output folder for processed text\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print(\"Folders ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69621066-4929-479d-ad66-9e7326f67a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    return \"\\n\".join((page.extract_text() or \"\") for page in reader.pages)\n",
    "\n",
    "\n",
    "def clean_project_text(raw_text: str) -> str:\n",
    "    # Normalize newlines\n",
    "    text = raw_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Remove pure page numbers\n",
    "    lines = []\n",
    "    for line in text.splitlines():\n",
    "        if re.fullmatch(r\"\\s*\\d+\\s*\", line):  # line is just a number\n",
    "            continue\n",
    "        lines.append(line.rstrip())\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    # 1Ô∏è‚É£ Try to start from the REAL \"1. INTRODUCTION\"\n",
    "    # We look for the exact string \"1. INTRODUCTION\" (case-sensitive) because\n",
    "    # TOC usually uses \"1. Introduction\" and body uses full caps.\n",
    "    idx = text.find(\"1. INTRODUCTION\")\n",
    "    if idx != -1:\n",
    "        text = text[idx:]\n",
    "    else:\n",
    "        # Fallback: if not found, keep whole text (e.g., FarmAuto style docs)\n",
    "        text = text.lstrip()\n",
    "\n",
    "    # 2Ô∏è‚É£ Drop REFERENCES / RELATED WORK / BIBLIOGRAPHY and everything after\n",
    "    drop_pattern = re.compile(\n",
    "        r\"(?im)^\\s*(REFERENCES|REFERENCE|RELATED WORKS?|BIBLIOGRAPHY)\\b.*$\",\n",
    "        re.MULTILINE,\n",
    "    )\n",
    "    match = drop_pattern.search(text)\n",
    "    if match:\n",
    "        text = text[:match.start()]\n",
    "\n",
    "    # 3Ô∏è‚É£ Collapse multiple blank lines\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    # 4Ô∏è‚É£ Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_pdf_folder(input_dir: str, output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pdf_paths = glob(os.path.join(input_dir, \"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_paths)} PDFs in {input_dir}\")\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        base = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        out_path = os.path.join(output_dir, f\"{base}_clean.txt\")\n",
    "\n",
    "        raw = extract_text_from_pdf(pdf_path)\n",
    "        cleaned = clean_project_text(raw)\n",
    "\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned)\n",
    "\n",
    "        print(f\"‚úÖ Saved cleaned: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01f45b6-c5f1-4391-b5cc-0c90e7a2b290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 PDFs in dataset\n",
      "‚úÖ Saved cleaned: clean_text/NotesTaker_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/TravelBuddy_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/AquaGuard_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/FarmAuto_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/ShopEase_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/RideSense_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/MovieStreaming_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/TrackFleet_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/HealthConnect_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/AgroVision_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/LearnMate_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/FoodOrder_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/HealthInsight_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/HomeSense_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/SmartFit_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/MedAlert_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/SalesPulse_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/QuizMaster_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/SmartDetect_clean.txt\n",
      "‚úÖ Saved cleaned: clean_text/SentimentFlow_clean.txt\n"
     ]
    }
   ],
   "source": [
    "preprocess_pdf_folder(INPUT_FOLDER, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb7d1ec-02e0-48a1-a0d5-cc40ea4a1354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. INTRODUCTION\n",
      "The demand for online food ordering has rapidly increased due to changing consumer preferences,\n",
      "time constraints, and the convenience of home delivery. Traditional dine-in and phone-based\n",
      "ordering methods are limited by communication gaps, long wait times, and lack of transparency in\n",
      "order tracking. Customers expect a seamless digital experience that allows them to browse menus,\n",
      "customize orders, and receive timely delivery updates.\n",
      "DineEasy is a web-based food ordering system designed to connect customers with restaurants\n",
      "through an intuitive online platform. The application enables users to browse menus, place orders,\n",
      "and track delivery status while providing restaurant owners with tools to manage orders and menu\n",
      "items effectively. The system aims to enhance ordering effi\n",
      "FarmAuto - Smart Irrigation and Resource Optimization System\n",
      "Page 1\n",
      "TITLE PAGE\n",
      "Project Title: FarmAuto - IoT Enabled Smart Irrigation and Resource Optimization\n",
      "System\n",
      "Prepared For: Government Agricultural Innovation and Rural Development\n",
      "Program\n",
      "Prepared By: [Startup or Team Name]\n",
      "Date: [Month, Year]\n",
      "\n",
      "FarmAuto - Smart Irrigation and Resource Optimization System\n",
      "Page 2\n",
      "1. EXECUTIVE SUMMARY\n",
      "FarmAuto is a smart irrigation and resource optimization system designed to\n",
      "support farmers by automating water distribution based on real time soil moisture\n",
      "and environmental conditions. The system reduces water wastage, improves crop\n",
      "yield consistency, and supports rural agricultural development using cost effective\n",
      "IoT technology. FarmAuto aligns with government goals of sustainable farming,\n",
      "digital tr\n"
     ]
    }
   ],
   "source": [
    "with open(\"clean_text/FoodOrder_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read()[:800])\n",
    "\n",
    "with open(\"clean_text/FarmAuto_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read()[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44319875-3ccb-4ec5-8a15-9561fb4f8d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean texts: 20\n",
      "JSON labels: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "CLEAN_TEXT_DIR = \"clean_text\"\n",
    "TRAIN_JSON_DIR = \"training_sample\"\n",
    "\n",
    "def normalize_stem(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize filename stem so txt/json can be matched.\n",
    "    Example: 'MovieStream_Project_clean' -> 'moviestream'\n",
    "             'movie_stream' -> 'moviestream'\n",
    "    \"\"\"\n",
    "    base = name.lower()\n",
    "    base = base.replace(\".txt\", \"\").replace(\".json\", \"\")\n",
    "    base = base.replace(\"_project\", \"\")\n",
    "    base = base.replace(\"_clean\", \"\")\n",
    "    base = base.replace(\" \", \"\")\n",
    "    base = base.replace(\"-\", \"\")\n",
    "    return base\n",
    "\n",
    "def load_clean_texts(clean_dir=CLEAN_TEXT_DIR):\n",
    "    txt_map = {}\n",
    "    for path in glob(os.path.join(clean_dir, \"*.txt\")):\n",
    "        stem = os.path.basename(path)\n",
    "        key = normalize_stem(stem)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            txt = f.read().strip()\n",
    "        txt_map[key] = {\"path\": path, \"text\": txt}\n",
    "    return txt_map\n",
    "\n",
    "def load_json_labels(json_dir=TRAIN_JSON_DIR):\n",
    "    json_map = {}\n",
    "    for path in glob(os.path.join(json_dir, \"*.json\")):\n",
    "        stem = os.path.basename(path)\n",
    "        key = normalize_stem(stem)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        json_map[key] = {\"path\": path, \"data\": data}\n",
    "    return json_map\n",
    "\n",
    "clean_map = load_clean_texts()\n",
    "json_map = load_json_labels()\n",
    "\n",
    "print(\"Clean texts:\", len(clean_map))\n",
    "print(\"JSON labels:\", len(json_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2149c68c-cc0a-4829-86ce-839cb2d788a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Matched 20 project(s) with both text and JSON.\n"
     ]
    }
   ],
   "source": [
    "training_pairs = []\n",
    "\n",
    "for key, clean_entry in clean_map.items():\n",
    "    if key not in json_map:\n",
    "        print(f\"‚ö†Ô∏è No JSON found for: {key} ({clean_entry['path']})\")\n",
    "        continue\n",
    "\n",
    "    label_entry = json_map[key]\n",
    "    project_text = clean_entry[\"text\"]\n",
    "    label_json = label_entry[\"data\"]\n",
    "\n",
    "    training_pairs.append({\n",
    "        \"key\": key,\n",
    "        \"project_text\": project_text,\n",
    "        \"label_json\": label_json\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Matched {len(training_pairs)} project(s) with both text and JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d3bc28-de55-4665-bdf1-24ed5132b3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('shopease',\n",
       " '1. INTRODUCTION\\nShopEase is a mobile application designed to address modern user needs through an intuitive and\\nefficient digital platform. The goal of the project is to provide a seamless and accessible experience\\nthat enables users to perform tasks conveniently from their smartphones.\\n2. PROBLEM STATEMENT\\nTraditional methods often result in inefficiencies, lack of centralization, and limited accessibility.\\nUsers require mobile-friendly solutions that offer real-time information, consistent per')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = training_pairs[1]\n",
    "example[\"key\"], example[\"project_text\"][:500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c651e47-f608-42d4-a0df-e3efe168d391",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf0090d-83be-470b-a4f9-63a7648dc463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== Title-only training prompts ==========\n",
    "\n",
    "import json\n",
    "\n",
    "def truncate(text: str, max_chars: int = 800) -> str:\n",
    "    \"\"\"\n",
    "    Shorten project text for training prompts.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    truncated = text[:max_chars]\n",
    "    last_space = truncated.rfind(\" \")\n",
    "    if last_space != -1:\n",
    "        truncated = truncated[:last_space]\n",
    "    return truncated\n",
    "\n",
    "# ---- EPIC TITLE TRAIN PROMPT ----\n",
    "EPIC_TITLE_TRAIN_PROMPT = \"\"\"\n",
    "Project description:\n",
    "{project_text}\n",
    "\n",
    "Task:\n",
    "- Write ONE short epic title that captures the main goal of the project.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY the epic title on a single line.\n",
    "- No labels, no quotes, no bullets, no numbering.\n",
    "\n",
    "Epic title:\n",
    "\"\"\"\n",
    "\n",
    "# ---- FEATURE TITLES TRAIN PROMPT ----\n",
    "FEATURE_TITLES_TRAIN_PROMPT = \"\"\"\n",
    "Project description:\n",
    "{project_text}\n",
    "\n",
    "Task:\n",
    "- Propose exactly {num_features} high-level feature titles that break the project\n",
    "  into major functional chunks.\n",
    "\n",
    "Rules:\n",
    "- Output EXACTLY {num_features} lines.\n",
    "- Each line must be ONE feature title.\n",
    "- No bullets, no numbering, no quotes.\n",
    "\n",
    "Feature titles (one per line):\n",
    "\"\"\"\n",
    "\n",
    "# ---- STORY TITLES TRAIN PROMPT ----\n",
    "STORY_TITLES_TRAIN_PROMPT = \"\"\"\n",
    "Project description:\n",
    "{project_text}\n",
    "\n",
    "Epic title:\n",
    "{epic_title}\n",
    "\n",
    "Feature title:\n",
    "{feature_title}\n",
    "\n",
    "Task:\n",
    "- Propose exactly {num_stories} user story titles for this feature.\n",
    "\n",
    "Rules:\n",
    "- Output EXACTLY {num_stories} lines.\n",
    "- Each line is a user story title.\n",
    "- Prefer concise titles (they may start with \"As a <role>, I want ...\" but are OPTIONAL).\n",
    "- No bullets, no numbering, no quotes.\n",
    "\n",
    "User story titles (one per line):\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1517814-3cd2-4f46-9f6a-90f8fa55ba6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 141\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "title_examples = []\n",
    "\n",
    "for pair in training_pairs:\n",
    "    project_text = pair[\"project_text\"]\n",
    "    label = deepcopy(pair[\"label_json\"])\n",
    "    \n",
    "    proj = truncate(project_text, max_chars=800)\n",
    "    epic_obj = label.get(\"epic\", {}) or {}\n",
    "    features = label.get(\"features\", []) or []\n",
    "\n",
    "    # ---------- EPIC TITLE EXAMPLE ----------\n",
    "    epic_title = str(epic_obj.get(\"title\", \"\")).strip()\n",
    "    if epic_title:\n",
    "        prompt = EPIC_TITLE_TRAIN_PROMPT.format(project_text=proj)\n",
    "        completion = epic_title  # ONE line, no labels\n",
    "        title_examples.append({\"text\": prompt + completion})\n",
    "\n",
    "    # ---------- FEATURE TITLES EXAMPLE ----------\n",
    "    feature_titles = []\n",
    "    for f in features:\n",
    "        t = str(f.get(\"title\", \"\")).strip()\n",
    "        if t:\n",
    "            feature_titles.append(t)\n",
    "\n",
    "    if feature_titles:\n",
    "        num_feats = len(feature_titles)\n",
    "        feat_prompt = FEATURE_TITLES_TRAIN_PROMPT.format(\n",
    "            project_text=proj,\n",
    "            num_features=num_feats,\n",
    "        )\n",
    "        feat_completion = \"\\n\".join(feature_titles)  # one title per line\n",
    "        title_examples.append({\"text\": feat_prompt + feat_completion})\n",
    "\n",
    "    # ---------- STORY TITLES EXAMPLES (ONE PER FEATURE) ----------\n",
    "    for f in features:\n",
    "        f_title = str(f.get(\"title\", \"\")).strip() or \"Feature\"\n",
    "        stories = f.get(\"user_stories\", []) or []\n",
    "\n",
    "        story_titles = []\n",
    "        for s in stories:\n",
    "            st = str(s.get(\"title\", \"\")).strip()\n",
    "            if st:\n",
    "                story_titles.append(st)\n",
    "\n",
    "        if not story_titles:\n",
    "            continue\n",
    "\n",
    "        num_stories = len(story_titles)\n",
    "        story_prompt = STORY_TITLES_TRAIN_PROMPT.format(\n",
    "            project_text=proj,\n",
    "            epic_title=epic_title if epic_title else \"Project Epic\",\n",
    "            feature_title=f_title,\n",
    "            num_stories=num_stories,\n",
    "        )\n",
    "        story_completion = \"\\n\".join(story_titles)\n",
    "        title_examples.append({\"text\": story_prompt + story_completion})\n",
    "\n",
    "print(f\"Total training examples: {len(title_examples)}\")\n",
    "\n",
    "dataset = Dataset.from_list(title_examples)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede514f-f7c5-475c-843a-0ced90a58fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47f39f97-1dad-4e14-92e8-e7abcc047adc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b3d013-9848-4c43-8b60-235998f79143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d5d072-85b6-45c9-9b46-cc7b206eed13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60af1c9c-54c3-4bff-a74c-282436f4a4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized samples: 141\n",
      "Example input_ids[:20]: [1, 29871, 13, 7653, 6139, 29901, 13, 29896, 29889, 19578, 1672, 14849, 9838, 13, 2111, 824, 14379, 5706, 2919, 18167]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "MAX_LEN = 1024  # keep it modest for GPU memory\n",
    "\n",
    "def tokenize_record(record):\n",
    "    # record[\"text\"] contains prompt + JSON target\n",
    "    encoded = tokenizer(\n",
    "        record[\"text\"],\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# Turn your python list `dataset` into a tokenized list\n",
    "tokenized_ds = [tokenize_record(r) for r in dataset]\n",
    "\n",
    "print(f\"Tokenized samples: {len(tokenized_ds)}\")\n",
    "print(\"Example input_ids[:20]:\", tokenized_ds[0][\"input_ids\"][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5539539-5cd4-4646-8867-ffea24e7f0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting dill==0.3.8\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: dill\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.16.1 requires dill<0.3.8,>=0.3.0, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mName: dill\n",
      "Version: 0.3.8\n",
      "Summary: serialize all of Python\n",
      "Home-page: https://github.com/uqfoundation/dill\n",
      "Author: Mike McKerns\n",
      "Author-email: mmckerns@uqfoundation.org\n",
      "License: BSD-3-Clause\n",
      "Location: /home/deshpande.sh/.local/lib/python3.10/site-packages\n",
      "Requires: \n",
      "Required-by: datasets, multiprocess\n"
     ]
    }
   ],
   "source": [
    "!pip install dill==0.3.8 --force-reinstall\n",
    "!pip show dill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07358cee-034f-4493-bac8-0c2e2434f7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.20437245579516677\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # standard for LLAMA/TinyLlama\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02b1a59-2245-44f8-9955-9f044cdfc490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def data_collator(features):\n",
    "    batch = {\n",
    "        \"input_ids\": [f[\"input_ids\"] for f in features],\n",
    "        \"attention_mask\": [f[\"attention_mask\"] for f in features],\n",
    "    }\n",
    "    batch[\"labels\"] = [f[\"input_ids\"] for f in features]\n",
    "    batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4749e8-62f3-4400-af8b-b0ea94b11907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"agileai_tinyllama_qlora\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,      \n",
    "    bf16=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48d8f63b-20e3-4ddf-b278-ce40faa24615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 04:27, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.830500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.611900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.578600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.519500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.411300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.200800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=350, training_loss=0.7595001772471837, metrics={'train_runtime': 268.6774, 'train_samples_per_second': 2.624, 'train_steps_per_second': 1.303, 'total_flos': 2375431068057600.0, 'train_loss': 0.7595001772471837, 'epoch': 4.96})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,  \n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3638f449-c3a5-4930-890b-cbfbcc87fe33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('agileai_tinyllama_qlora_v4/tokenizer_config.json',\n",
       " 'agileai_tinyllama_qlora_v4/special_tokens_map.json',\n",
       " 'agileai_tinyllama_qlora_v4/tokenizer.model',\n",
       " 'agileai_tinyllama_qlora_v4/added_tokens.json',\n",
       " 'agileai_tinyllama_qlora_v4/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_DIR = \"agileai_tinyllama_qlora_v4\"   # or similar\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bcc8a0-0204-4cc8-a78a-14b7d31a81c0",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65298a79-e99e-4d71-b28b-1351796f930f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded trained AgileAI model from: agileai_tinyllama_qlora_v4\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# üî¥ DO NOT rely on SAVE_DIR if you restarted the kernel\n",
    "# üëá Put the actual folder name you used in training:\n",
    "MODEL_DIR = \"agileai_tinyllama_qlora_v4\"   # or \"agileai_tinyllama_qlora\", etc.\n",
    "\n",
    "assert os.path.isdir(MODEL_DIR), f\"Model directory not found: {MODEL_DIR}\"\n",
    "\n",
    "gen_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "if gen_tokenizer.pad_token is None:\n",
    "    gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "\n",
    "gen_model.eval()\n",
    "print(\"‚úÖ Loaded trained AgileAI model from:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30b8e9c2-f95d-4d81-894e-a27c212140b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 1: helpers & cleaning\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# --- Truncate helper ---\n",
    "\n",
    "def truncate(text: str, max_chars: int = 800) -> str:\n",
    "    \"\"\"\n",
    "    Safe truncation for project text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    truncated = text[:max_chars]\n",
    "    last_space = truncated.rfind(\" \")\n",
    "    if last_space != -1:\n",
    "        truncated = truncated[:last_space]\n",
    "    return truncated\n",
    "\n",
    "# --- Clean title lines ---\n",
    "\n",
    "def _clean_title_line(line: str) -> str:\n",
    "    line = line.strip()\n",
    "    # remove bullets / numbering\n",
    "    line = re.sub(r'^[\\-\\*\\d\\.\\)\\s]+', '', line)\n",
    "    # remove \"Feature X:\" / \"Story X:\" prefixes\n",
    "    line = re.sub(r'^Feature\\s*\\d+\\s*:\\s*', '', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'^Story\\s*\\d+\\s*:\\s*',   '', line, flags=re.IGNORECASE)\n",
    "    # strip quotes\n",
    "    line = line.strip('\"‚Äú‚Äù ').strip()\n",
    "    return line\n",
    "\n",
    "def limit_words(text: str, max_words: int) -> str:\n",
    "    words = text.strip().split()\n",
    "    if len(words) <= max_words:\n",
    "        return text.strip()\n",
    "    return \" \".join(words[:max_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31de196d-f6c4-4502-b535-eaa7831da97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Cell 2: reuse training prompt templates\n",
    "# =====================================\n",
    "\n",
    "# These should already exist from your training cells:\n",
    "# EPIC_TITLE_TRAIN_PROMPT\n",
    "# FEATURE_TITLES_TRAIN_PROMPT\n",
    "# STORY_TITLES_TRAIN_PROMPT\n",
    "\n",
    "EPIC_TITLE_PROMPT_TEXT     = EPIC_TITLE_TRAIN_PROMPT\n",
    "FEATURE_TITLES_PROMPT_TEXT = FEATURE_TITLES_TRAIN_PROMPT\n",
    "STORY_TITLES_PROMPT_TEXT   = STORY_TITLES_TRAIN_PROMPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ee8fdd2-5073-4874-82b9-de7c9740702d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 3: core text generation + confidence\n",
    "# ===========================================\n",
    "\n",
    "def generate_text_and_confidence(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 80,\n",
    "    do_sample: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      completion (str),\n",
    "      confidence (float in [0,1]),\n",
    "      mean_log_prob (float)\n",
    "    \"\"\"\n",
    "    # 1) Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if decoded.startswith(prompt):\n",
    "        completion = decoded[len(prompt):].strip()\n",
    "    else:\n",
    "        completion = decoded.strip()\n",
    "\n",
    "    # 2) Recompute log-probs over prompt + completion\n",
    "    full_text = prompt + completion\n",
    "    enc = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc, labels=enc[\"input_ids\"])\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    shift_logits = logits[:, :-1, :]\n",
    "    shift_labels = enc[\"input_ids\"][:, 1:]\n",
    "\n",
    "    log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # only consider tokens after the prompt\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "    gen_token_log_probs = token_log_probs[0, prompt_len-1:]\n",
    "\n",
    "    if gen_token_log_probs.numel() == 0:\n",
    "        mean_log_prob = -5.0\n",
    "    else:\n",
    "        mean_log_prob = gen_token_log_probs.mean().item()\n",
    "\n",
    "    confidence = float(torch.sigmoid(torch.tensor(mean_log_prob)))\n",
    "\n",
    "    return completion, float(confidence), float(mean_log_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "884d35bc-c2c4-494c-8bae-eafafe4271fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 4: epic, features, and stories titles\n",
    "# ===========================================\n",
    "\n",
    "def generate_epic_title(project_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        \"id\": \"E1\",\n",
    "        \"title\": \"...\",\n",
    "        \"confidence\": float,\n",
    "        \"mean_log_prob\": float\n",
    "      }\n",
    "    \"\"\"\n",
    "    proj = truncate(project_text, max_chars=800)\n",
    "    prompt = EPIC_TITLE_PROMPT_TEXT.format(project_text=proj)\n",
    "\n",
    "    raw, conf, mean_lp = generate_text_and_confidence(\n",
    "        prompt,\n",
    "        max_new_tokens=40,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    lines = [l for l in raw.splitlines() if l.strip()]\n",
    "    if not lines:\n",
    "        epic_title = \"Project Epic\"\n",
    "    else:\n",
    "        epic_title = _clean_title_line(lines[0])\n",
    "\n",
    "    epic_title = limit_words(epic_title, max_words=10) or \"Project Epic\"\n",
    "\n",
    "    return {\n",
    "        \"id\": \"E1\",\n",
    "        \"title\": epic_title,\n",
    "        \"confidence\": conf,\n",
    "        \"mean_log_prob\": mean_lp,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_feature_titles(\n",
    "    project_text: str,\n",
    "    epic_obj: dict,\n",
    "    num_features: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      feature_dicts: list of {\n",
    "         \"id\",\"title\",\"confidence\",\"mean_log_prob\",\"user_stories\":[]\n",
    "      }\n",
    "      feat_conf: float\n",
    "      feat_mean_lp: float\n",
    "    \"\"\"\n",
    "    proj = truncate(project_text, max_chars=800)\n",
    "\n",
    "    prompt = FEATURE_TITLES_PROMPT_TEXT.format(\n",
    "        project_text=proj,\n",
    "        num_features=num_features,\n",
    "    )\n",
    "\n",
    "    raw, conf, mean_lp = generate_text_and_confidence(\n",
    "        prompt,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    lines = [l for l in raw.splitlines() if l.strip()]\n",
    "    titles = []\n",
    "\n",
    "    for l in lines:\n",
    "        t = _clean_title_line(l)\n",
    "        if t:\n",
    "            t = limit_words(t, max_words=10)\n",
    "            titles.append(t)\n",
    "\n",
    "    # enforce exactly num_features\n",
    "    if len(titles) < num_features:\n",
    "        for i in range(len(titles) + 1, num_features + 1):\n",
    "            titles.append(f\"Feature {i}\")\n",
    "    else:\n",
    "        titles = titles[:num_features]\n",
    "\n",
    "    feature_dicts = []\n",
    "    for i, t in enumerate(titles, start=1):\n",
    "        feature_dicts.append({\n",
    "            \"id\": f\"F{i}\",\n",
    "            \"title\": t,\n",
    "            \"confidence\": conf,\n",
    "            \"mean_log_prob\": mean_lp,\n",
    "            \"user_stories\": [],\n",
    "        })\n",
    "\n",
    "    return feature_dicts, conf, mean_lp\n",
    "\n",
    "\n",
    "def generate_story_titles_for_feature(\n",
    "    project_text: str,\n",
    "    epic_obj: dict,\n",
    "    feature_obj: dict,\n",
    "    num_stories: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      stories: list of {\n",
    "        \"id\",\"title\",\"confidence\",\"mean_log_prob\"\n",
    "      }\n",
    "      s_conf: float\n",
    "      s_mean_lp: float\n",
    "    \"\"\"\n",
    "    proj = truncate(project_text, max_chars=800)\n",
    "    epic_title = epic_obj.get(\"title\", \"Project Epic\")\n",
    "    feature_title = feature_obj.get(\"title\", \"Feature\")\n",
    "\n",
    "    prompt = STORY_TITLES_PROMPT_TEXT.format(\n",
    "        project_text=proj,\n",
    "        epic_title=epic_title,\n",
    "        feature_title=feature_title,\n",
    "        num_stories=num_stories,\n",
    "    )\n",
    "\n",
    "    raw, conf, mean_lp = generate_text_and_confidence(\n",
    "        prompt,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    lines = [l for l in raw.splitlines() if l.strip()]\n",
    "    titles = []\n",
    "\n",
    "    for l in lines:\n",
    "        t = _clean_title_line(l)\n",
    "        if not t:\n",
    "            continue\n",
    "        t = limit_words(t, max_words=15)\n",
    "        titles.append(t)\n",
    "\n",
    "    if len(titles) < num_stories:\n",
    "        for i in range(len(titles) + 1, num_stories + 1):\n",
    "            titles.append(f\"Story {i}: basic capability for {feature_title.lower()}\")\n",
    "    else:\n",
    "        titles = titles[:num_stories]\n",
    "\n",
    "    stories = []\n",
    "    # e.g. F3 -> \"3\" for user story ID\n",
    "    feature_index = feature_obj[\"id\"][1:] if len(feature_obj[\"id\"]) > 1 else \"1\"\n",
    "\n",
    "    for j, st in enumerate(titles, start=1):\n",
    "        stories.append({\n",
    "            \"id\": f\"US{feature_index}_{j}\",\n",
    "            \"title\": st,\n",
    "            \"confidence\": conf,\n",
    "            \"mean_log_prob\": mean_lp,\n",
    "        })\n",
    "\n",
    "    return stories, conf, mean_lp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7b6edd2-1be6-4bdf-a2bb-c0ce2ba9cbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 5: metrics helpers (separate report)\n",
    "# ===========================================\n",
    "\n",
    "def compute_pseudo_perplexity_over_titles(model, tokenizer, titles_text: str):\n",
    "    \"\"\"\n",
    "    Pseudo-perplexity over concatenated titles.\n",
    "    \"\"\"\n",
    "    titles_text = (titles_text or \"\").strip()\n",
    "    if not titles_text:\n",
    "        return {\n",
    "            \"mean_log_prob_titles\": float(\"nan\"),\n",
    "            \"pseudo_perplexity_titles\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    inputs = tokenizer(titles_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "\n",
    "    ppl = math.exp(neg_log_likelihood)\n",
    "    return {\n",
    "        \"mean_log_prob_titles\": -neg_log_likelihood,\n",
    "        \"pseudo_perplexity_titles\": ppl,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_structure_ratios(features, expected_features=5, expected_stories=3):\n",
    "    num_features = len(features)\n",
    "    feature_ratio = num_features / expected_features if expected_features > 0 else 0.0\n",
    "\n",
    "    story_ratios = []\n",
    "    for f in features:\n",
    "        stories = f.get(\"user_stories\", [])\n",
    "        story_ratios.append(\n",
    "            len(stories) / expected_stories if expected_stories > 0 else 0.0\n",
    "        )\n",
    "\n",
    "    story_count_ratio = (\n",
    "        sum(story_ratios) / len(story_ratios) if story_ratios else 0.0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"feature_count_ratio\": feature_ratio,\n",
    "        \"story_count_ratio\": story_count_ratio,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_repetition_rates_for_titles(titles_text: str):\n",
    "    tokens = titles_text.split()\n",
    "    if len(tokens) < 2:\n",
    "        return {\n",
    "            \"bigram_repetition_rate\": 0.0,\n",
    "            \"trigram_repetition_rate\": 0.0,\n",
    "        }\n",
    "\n",
    "    bigrams = [\" \".join(tokens[i:i+2]) for i in range(len(tokens) - 1)]\n",
    "    trigrams = [\" \".join(tokens[i:i+3]) for i in range(len(tokens) - 2)] if len(tokens) >= 3 else []\n",
    "\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    trigram_counts = Counter(trigrams)\n",
    "\n",
    "    repeated_bigrams = sum(1 for _, v in bigram_counts.items() if v > 1)\n",
    "    repeated_trigrams = sum(1 for _, v in trigram_counts.items() if v > 1)\n",
    "\n",
    "    bigram_rep_rate = repeated_bigrams / max(1, len(bigram_counts))\n",
    "    trigram_rep_rate = (\n",
    "        repeated_trigrams / max(1, len(trigram_counts)) if trigram_counts else 0.0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bigram_repetition_rate\": bigram_rep_rate,\n",
    "        \"trigram_repetition_rate\": trigram_rep_rate,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9582eeca-5b0b-4671-a3ce-1ceadc6ccf7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Cell 6: main inference runner + metrics report\n",
    "# ===============================================\n",
    "\n",
    "def run_agileai_titles_only_with_report(\n",
    "    project_text: str,\n",
    "    num_features: int = 5,\n",
    "    stories_per_feature: int = 3,\n",
    "):\n",
    "    # 1. Epic\n",
    "    epic = generate_epic_title(project_text)\n",
    "\n",
    "    # 2. Features\n",
    "    feature_dicts, feat_conf, feat_mean_lp = generate_feature_titles(\n",
    "        project_text,\n",
    "        epic,\n",
    "        num_features=num_features,\n",
    "    )\n",
    "\n",
    "    story_block_confs = []\n",
    "    story_block_lps = []\n",
    "\n",
    "    for feat in feature_dicts:\n",
    "        stories, s_conf, s_lp = generate_story_titles_for_feature(\n",
    "            project_text,\n",
    "            epic,\n",
    "            feat,\n",
    "            num_stories=stories_per_feature,\n",
    "        )\n",
    "        feat[\"user_stories\"] = stories\n",
    "        story_block_confs.append(s_conf)\n",
    "        story_block_lps.append(s_lp)\n",
    "\n",
    "    # ---------- Clean Agile output (titles only) ----------\n",
    "    clean_epic = {\n",
    "        \"id\": epic[\"id\"],\n",
    "        \"title\": epic[\"title\"],\n",
    "    }\n",
    "\n",
    "    clean_features = []\n",
    "    for f in feature_dicts:\n",
    "        clean_stories = [\n",
    "            {\"id\": us[\"id\"], \"title\": us[\"title\"]}\n",
    "            for us in f.get(\"user_stories\", [])\n",
    "        ]\n",
    "        clean_features.append({\n",
    "            \"id\": f[\"id\"],\n",
    "            \"title\": f[\"title\"],\n",
    "            \"user_stories\": clean_stories,\n",
    "        })\n",
    "\n",
    "    agile_output = {\n",
    "        \"epic\": clean_epic,\n",
    "        \"features\": clean_features,\n",
    "    }\n",
    "\n",
    "    # ---------- Metrics report ----------\n",
    "    def _safe_avg(vals):\n",
    "        return float(sum(vals) / len(vals)) if vals else 0.0\n",
    "\n",
    "    confidence_metrics = {\n",
    "        \"epic_confidence\": epic.get(\"confidence\", 0.0),\n",
    "        \"epic_mean_log_prob\": epic.get(\"mean_log_prob\", 0.0),\n",
    "        \"features_block_confidence\": feat_conf,\n",
    "        \"features_block_mean_log_prob\": feat_mean_lp,\n",
    "        \"stories_avg_block_confidence\": _safe_avg(story_block_confs),\n",
    "        \"stories_avg_block_mean_log_prob\": _safe_avg(story_block_lps),\n",
    "    }\n",
    "\n",
    "    # Flat titles text\n",
    "    all_titles = [clean_epic[\"title\"]]\n",
    "    for f in clean_features:\n",
    "        all_titles.append(f[\"title\"])\n",
    "        for us in f[\"user_stories\"]:\n",
    "            all_titles.append(us[\"title\"])\n",
    "    titles_text = \" \".join(all_titles)\n",
    "\n",
    "    ppl_metrics = compute_pseudo_perplexity_over_titles(\n",
    "        model, tokenizer, titles_text\n",
    "    )\n",
    "    struct_metrics = compute_structure_ratios(\n",
    "        clean_features,\n",
    "        expected_features=num_features,\n",
    "        expected_stories=stories_per_feature,\n",
    "    )\n",
    "    rep_metrics = compute_repetition_rates_for_titles(titles_text)\n",
    "\n",
    "    metrics_report = {\n",
    "        **confidence_metrics,\n",
    "        **ppl_metrics,\n",
    "        **struct_metrics,\n",
    "        **rep_metrics,\n",
    "    }\n",
    "\n",
    "    return agile_output, metrics_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03a30b54-1e3b-4155-83e1-eefde4621d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGILE OUTPUT ===\n",
      "{\n",
      "  \"epic\": {\n",
      "    \"id\": \"E1\",\n",
      "    \"title\": \"SmartDetect Anomaly Detection and Edge Deployment System\"\n",
      "  },\n",
      "  \"features\": [\n",
      "    {\n",
      "      \"id\": \"F1\",\n",
      "      \"title\": \"Data Preprocessing and Feature Extraction\",\n",
      "      \"user_stories\": [\n",
      "        {\n",
      "          \"id\": \"US1_1\",\n",
      "          \"title\": \"Preprocess Input Data for Feature Extraction\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US1_2\",\n",
      "          \"title\": \"Process Encoded Input Data for Feature Extraction\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US1_3\",\n",
      "          \"title\": \"Extract Feature Values from Encoded Input Data\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"F2\",\n",
      "      \"title\": \"Model Training and Evaluation\",\n",
      "      \"user_stories\": [\n",
      "        {\n",
      "          \"id\": \"US2_1\",\n",
      "          \"title\": \"Train Model\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US2_2\",\n",
      "          \"title\": \"Evaluate Model\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US2_3\",\n",
      "          \"title\": \"Deploy Model to Edge Device\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"F3\",\n",
      "      \"title\": \"Edge Deployment and Testing\",\n",
      "      \"user_stories\": [\n",
      "        {\n",
      "          \"id\": \"US3_1\",\n",
      "          \"title\": \"Deploy SmartDetect to a Single Edge Device\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US3_2\",\n",
      "          \"title\": \"Test SmartDetect on a Single Edge Device\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US3_3\",\n",
      "          \"title\": \"Configure SmartDetect for Multiple Edge Devices\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"F4\",\n",
      "      \"title\": \"Inference Performance and Deployment\",\n",
      "      \"user_stories\": [\n",
      "        {\n",
      "          \"id\": \"US4_1\",\n",
      "          \"title\": \"Evaluate Model Accuracy and Performance\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US4_2\",\n",
      "          \"title\": \"Deploy Model to Edge Device\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US4_3\",\n",
      "          \"title\": \"Support Real Time Alerting\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"F5\",\n",
      "      \"title\": \"Documentation, Integration, and Deployment\",\n",
      "      \"user_stories\": [\n",
      "        {\n",
      "          \"id\": \"US5_1\",\n",
      "          \"title\": \"Configure SmartDetect Model and Edge Device\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US5_2\",\n",
      "          \"title\": \"Integrate SmartDetect Model into Network\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"US5_3\",\n",
      "          \"title\": \"Deploy SmartDetect Model to Edge Device\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "=== METRICS REPORT ===\n",
      "{\n",
      "  \"epic_confidence\": 0.449150025844574,\n",
      "  \"epic_mean_log_prob\": -0.20410555601119995,\n",
      "  \"features_block_confidence\": 0.3623674511909485,\n",
      "  \"features_block_mean_log_prob\": -0.5651034712791443,\n",
      "  \"stories_avg_block_confidence\": 0.332436865568161,\n",
      "  \"stories_avg_block_mean_log_prob\": -0.6988651275634765,\n",
      "  \"mean_log_prob_titles\": -2.1816864013671875,\n",
      "  \"pseudo_perplexity_titles\": 8.861237267988784,\n",
      "  \"feature_count_ratio\": 1.0,\n",
      "  \"story_count_ratio\": 1.0,\n",
      "  \"bigram_repetition_rate\": 0.225,\n",
      "  \"trigram_repetition_rate\": 0.09473684210526316\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Cell 7: test run on a project\n",
    "# ==============================\n",
    "\n",
    "import json\n",
    "\n",
    "# Example: use one of your training projects, or a cleaned new project description\n",
    "proj_text = training_pairs[0][\"project_text\"]  # or your cleaned text from PDF\n",
    "\n",
    "agile_output, metrics_report = run_agileai_titles_only_with_report(\n",
    "    proj_text,\n",
    "    num_features=5,\n",
    "    stories_per_feature=3,\n",
    ")\n",
    "\n",
    "print(\"=== AGILE OUTPUT ===\")\n",
    "print(json.dumps(agile_output, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\n=== METRICS REPORT ===\")\n",
    "print(json.dumps(metrics_report, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a93972-237f-4501-84f7-3b59acbcb586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
