{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyMZxcARqQJfZQlikBFkuqGM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrimayee30/AgileAI/blob/main/playground1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tob1IyooKQnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AgileAI"
      ],
      "metadata": {
        "id": "Gs8tfOhOKrt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing\n"
      ],
      "metadata": {
        "id": "RWPQWYR4z40Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Data Preprocessing - Cell 1\n",
        "# Setup: installs, Drive, paths\n",
        "# ============================\n",
        "\n",
        "# Install PDF libraries for Colab\n",
        "!pip install -q pypdf pdfminer.six\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Core imports\n",
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "from pypdf import PdfReader\n",
        "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
        "\n",
        "# ---- Project paths (adjusted to your structure) ----\n",
        "\n",
        "# Root folder for your AgileAI Part 1 project in Drive\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/AgileAI/Part 1\")\n",
        "\n",
        "# Folder where your raw PDFs live\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "\n",
        "# Folder where we will save cleaned text + artifacts\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"preprocessed\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Main input PDF for now\n",
        "PDF_PATH = DATA_DIR / \"Website - E commerce fashion.pdf\"\n",
        "\n",
        "assert PDF_PATH.exists(), f\"PDF not found: {PDF_PATH}\"\n",
        "print(f\"Using input PDF: {PDF_PATH}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXsybrj00HiN",
        "outputId": "4b921fed-20b1-4268-bad0-a8edf9614733"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using input PDF: /content/drive/MyDrive/AgileAI/Part 1/data/Website - E commerce fashion.pdf\n",
            "Output directory: /content/drive/MyDrive/AgileAI/Part 1/preprocessed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 2 — Helpers + PDF → raw text\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "from io import StringIO\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "\n",
        "# Work inside your project folder so outputs land in AgileAI/Part 1\n",
        "os.chdir(PROJECT_ROOT)\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# ---- Simple helpers to read/write text ----\n",
        "\n",
        "def write_text(p: Path, s: str):\n",
        "    p.write_text(s, encoding=\"utf-8\")\n",
        "\n",
        "def read_text(p: Path) -> str:\n",
        "    return p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "# ---- PDF extraction: PyPDF primary, pdfminer fallback ----\n",
        "\n",
        "def extract_text_pypdf(path: Path) -> str:\n",
        "    reader = PdfReader(str(path))\n",
        "    pages = []\n",
        "    for p in reader.pages:\n",
        "        try:\n",
        "            pages.append(p.extract_text() or \"\")\n",
        "        except Exception:\n",
        "            pages.append(\"\")\n",
        "    return \"\\n\\n\".join(pages)\n",
        "\n",
        "def extract_text_pdfminer(path: Path) -> str:\n",
        "    out = StringIO()\n",
        "    with open(path, \"rb\") as f:\n",
        "        extract_text_to_fp(f, out)\n",
        "    return out.getvalue()\n",
        "\n",
        "def extract_pdf_text(path: Path) -> str:\n",
        "    \"\"\"\n",
        "    Try PyPDF first; if that fails or is empty, fall back to pdfminer.\n",
        "    Returns one big raw text string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        raw = extract_text_pypdf(path)\n",
        "        if not raw.strip():\n",
        "            raise ValueError(\"PyPDF returned empty text\")\n",
        "        print(\"Extracted text using PyPDF.\")\n",
        "    except Exception as e:\n",
        "        print(\"PyPDF failed or was empty, falling back to pdfminer:\", e)\n",
        "        raw = extract_text_pdfminer(path)\n",
        "        print(\"Extracted text using pdfminer.\")\n",
        "    return raw\n",
        "\n",
        "\n",
        "# ---- Extract and save raw text for debugging ----\n",
        "\n",
        "raw_text = extract_pdf_text(PDF_PATH)\n",
        "\n",
        "RAW_TEXT_PATH = PROJECT_ROOT / \"raw_text_from_pdf.txt\"\n",
        "write_text(RAW_TEXT_PATH, raw_text)\n",
        "\n",
        "print(f\"Raw text length: {len(raw_text)} characters\")\n",
        "print(\"Saved raw text to:\", RAW_TEXT_PATH)\n",
        "print(\"\\n--- RAW TEXT PREVIEW (first 500 chars) ---\\n\")\n",
        "print(raw_text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeigcuFq0rmc",
        "outputId": "68f12a20-ddc7-4127-8f64-ada1070a401b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/MyDrive/AgileAI/Part 1\n",
            "Extracted text using PyPDF.\n",
            "Raw text length: 16085 characters\n",
            "Saved raw text to: /content/drive/MyDrive/AgileAI/Part 1/raw_text_from_pdf.txt\n",
            "\n",
            "--- RAW TEXT PREVIEW (first 500 chars) ---\n",
            "\n",
            "Governors State University\n",
            "OPUS Open Portal to University Scholarship\n",
            "All Capstone Projects Student Capstone Projects\n",
            "Summer 2014\n",
            "Design and Implementation of E-Commerce Site\n",
            "for Online Shopping\n",
            "Sidhartha Reddy Vatrapu\n",
            "Governors State University\n",
            "Follow this and additional works at: https://opus.govst.edu/capstones\n",
            "Part of the Databases and Information Systems Commons, and the Systems Architecture\n",
            "Commons\n",
            "For more information about the academic degree, extended learning, and certificate programs \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 3 — Low-level cleaning helpers\n",
        "# =====================================\n",
        "\n",
        "def remove_nonprintable_and_glyphs(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Fix common unicode/glyph issues and drop non-printable chars.\n",
        "    \"\"\"\n",
        "    replacements = {\n",
        "        '\\xa0': ' ',   # non-breaking space\n",
        "        '\\u2018': \"'\", # left single quote\n",
        "        '\\u2019': \"'\", # right single quote\n",
        "        '\\u201c': '\"', # left double quote\n",
        "        '\\u201d': '\"', # right double quote\n",
        "        '\\u2013': '-', # en dash\n",
        "        '\\u2014': '-', # em dash\n",
        "        '\\uf0d8': '-', # bullet-like glyph\n",
        "        '\\uf0b7': '-', # bullet-like glyph\n",
        "        '\\u2022': '-', # bullet\n",
        "    }\n",
        "    for bad, good in replacements.items():\n",
        "        text = text.replace(bad, good)\n",
        "\n",
        "    # Keep only printable chars + newlines\n",
        "    clean = ''.join(ch for ch in text if ch.isprintable() or ch == '\\n')\n",
        "    return clean\n",
        "\n",
        "def remove_figure_placeholders(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove figure tags, captions, and similar noise.\n",
        "    \"\"\"\n",
        "    patterns = [\n",
        "        r\"<figure>.*?</figure>\",\n",
        "        r\"<figcaption>.*?</figcaption>\",\n",
        "        r\"Figure\\s*\\d+[:\\-]?.*?$\",\n",
        "        r\"Copyright[^\\\\n]*\",\n",
        "        r\"PageNumber=\\\"\\d+\\\"\",\n",
        "        r\"<!--.*?-->\",\n",
        "    ]\n",
        "    for pat in patterns:\n",
        "        text = re.sub(pat, \"\", text, flags=re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "def canonicalize_bullets_and_lists(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert many bullet characters to '-', normalize numbered list format.\n",
        "    \"\"\"\n",
        "    # Replace various bullet-like characters with '-'\n",
        "    text = re.sub(r\"[•\\u2022\\u2023\\u25E6\\u2043\\u2219\\uf0d8\\uf0b7\\*]\", \"-\", text)\n",
        "    text = re.sub(r\"[·•]\", \"-\", text)\n",
        "\n",
        "    # Normalize numbered lists: \"1. Something\" -> \"1. Something\"\n",
        "    text = re.sub(r\"^(\\d+)\\.\\s*\", r\"\\1. \", text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "def remove_repeated_footers(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Detect lines that repeat many times (likely headers/footers) and remove them.\n",
        "    \"\"\"\n",
        "    lines = text.splitlines()\n",
        "    counts: Dict[str, int] = {}\n",
        "    for ln in lines:\n",
        "        ln_s = ln.strip()\n",
        "        if not ln_s:\n",
        "            continue\n",
        "        counts[ln_s] = counts.get(ln_s, 0) + 1\n",
        "\n",
        "    # Anything that appears 2+ times is suspicious\n",
        "    repeated = {ln for ln, c in counts.items() if c >= 2}\n",
        "    if repeated:\n",
        "        pattern = \"(\" + \"|\".join(re.escape(r) for r in repeated) + \")\"\n",
        "        text = re.sub(pattern, \"\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def normalize_whitespace(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize spaces + newlines:\n",
        "      - collapse multiple spaces/tabs\n",
        "      - collapse 3+ blank lines into 2\n",
        "      - trim leading/trailing blank lines\n",
        "    \"\"\"\n",
        "    # Collapse long runs of spaces/tabs\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "\n",
        "    # Collapse 3+ newlines into 2\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "\n",
        "    # Strip blank lines at start/end\n",
        "    lines = [ln.strip() for ln in text.splitlines()]\n",
        "    while lines and not lines[0]:\n",
        "        lines.pop(0)\n",
        "    while lines and not lines[-1]:\n",
        "        lines.pop()\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ],
      "metadata": {
        "id": "MN8Fane00-4U"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 4 — Boilerplate + TOC removal + full cleaning\n",
        "# =====================================\n",
        "\n",
        "from typing import List, Tuple\n",
        "\n",
        "def truncate_boilerplate_start(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Try to skip front matter and jump to ABSTRACT / INTRODUCTION section.\n",
        "    \"\"\"\n",
        "    markers = [\n",
        "        r\"\\bABSTRACT\\b\",\n",
        "        r\"\\b1\\.0\\s+INTRODUCTION\\b\",\n",
        "        r\"\\bDesign and Implementation of E-Commerce Site\\b\",\n",
        "    ]\n",
        "    first_idx = None\n",
        "    for m in markers:\n",
        "        match = re.search(m, text, flags=re.IGNORECASE)\n",
        "        if match:\n",
        "            idx = match.start()\n",
        "            if first_idx is None or idx < first_idx:\n",
        "                first_idx = idx\n",
        "\n",
        "    if first_idx is not None:\n",
        "        return text[first_idx:]\n",
        "\n",
        "    # Fallback: scan first ~120 lines for 1.0 / ABSTRACT / INTRODUCTION\n",
        "    lines = text.splitlines()\n",
        "    for i, ln in enumerate(lines[:120]):\n",
        "        if re.match(r'^\\s*(1\\.0|ABSTRACT|INTRODUCTION)', ln, flags=re.IGNORECASE):\n",
        "            return \"\\n\".join(lines[i:])\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_toc_blocks(lines: List[str], min_block_len: int = 3) -> Tuple[List[str], List[Tuple[int, int]]]:\n",
        "    \"\"\"\n",
        "    Remove contiguous blocks of TOC-like lines.\n",
        "    Returns (filtered_lines, ranges_removed).\n",
        "    \"\"\"\n",
        "\n",
        "    def is_toc_line(ln: str) -> bool:\n",
        "        s = ln.strip()\n",
        "        if not s:\n",
        "            return False\n",
        "        # pattern like \"4.1  Intro........ 12\"\n",
        "        if re.search(r\"\\.{3,}\\s*\\d+\\s*$\", s):\n",
        "            return True\n",
        "        if re.match(r\"^\\s*(TABLE OF CONTENTS|CONTENTS|INDEX)\\b\", s, flags=re.IGNORECASE):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    n = len(lines)\n",
        "    toc_ranges: List[Tuple[int, int]] = []\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        if is_toc_line(lines[i]):\n",
        "            j = i\n",
        "            while j < n and is_toc_line(lines[j]):\n",
        "                j += 1\n",
        "            if j - i >= min_block_len:\n",
        "                toc_ranges.append((i, j))\n",
        "                i = j\n",
        "                continue\n",
        "        i += 1\n",
        "\n",
        "    # Fallback: single TABLE OF CONTENTS mention near top\n",
        "    if not toc_ranges:\n",
        "        for idx, ln in enumerate(lines[:200]):\n",
        "            if re.search(r\"\\b(table of contents|contents|index)\\b\", ln, flags=re.IGNORECASE):\n",
        "                end_idx = None\n",
        "                for j in range(idx + 1, min(len(lines), idx + 60)):\n",
        "                    if re.search(r\"\\b(1\\.0|introduction|abstract)\\b\", lines[j], flags=re.IGNORECASE):\n",
        "                        end_idx = j\n",
        "                        break\n",
        "                if end_idx is None:\n",
        "                    end_idx = min(len(lines), idx + 40)\n",
        "                if end_idx - idx >= 2:\n",
        "                    toc_ranges.append((idx, end_idx))\n",
        "                break\n",
        "\n",
        "    kept = []\n",
        "    removed_idx = set()\n",
        "    for start, end in toc_ranges:\n",
        "        for k in range(start, end):\n",
        "            removed_idx.add(k)\n",
        "    for idx, ln in enumerate(lines):\n",
        "        if idx not in removed_idx:\n",
        "            kept.append(ln)\n",
        "\n",
        "    return kept, toc_ranges\n",
        "\n",
        "def drop_residual_toc_fragments(lines: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Drop leftover TOC-like lines (dots + page numbers, etc.).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for ln in lines:\n",
        "        s = ln.strip()\n",
        "        if re.search(r\"\\.{3,}\\s*\\d+\\s*$\", s):\n",
        "            continue\n",
        "        if re.match(r\"^\\s*(table of contents|contents|index)\\b\", s, flags=re.IGNORECASE):\n",
        "            continue\n",
        "        filtered.append(ln)\n",
        "    return filtered\n",
        "\n",
        "def clean_text_pipeline_with_toc_removal(raw_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Full cleaning pipeline:\n",
        "      1) glyph cleanup\n",
        "      2) truncate front boilerplate\n",
        "      3) remove figures\n",
        "      4) remove repeated footers\n",
        "      5) normalize bullets/lists\n",
        "      6) whitespace normalization\n",
        "      7) remove TOC blocks + leftovers\n",
        "      8) final whitespace cleanup\n",
        "    \"\"\"\n",
        "    t = remove_nonprintable_and_glyphs(raw_text)\n",
        "    t = truncate_boilerplate_start(t)\n",
        "    t = remove_figure_placeholders(t)\n",
        "    t = remove_repeated_footers(t)\n",
        "    t = canonicalize_bullets_and_lists(t)\n",
        "    t = normalize_whitespace(t)\n",
        "\n",
        "    lines = t.splitlines()\n",
        "    filtered, removed = remove_toc_blocks(lines)\n",
        "    if removed:\n",
        "        print(\"Removed TOC/index block ranges:\", removed)\n",
        "    filtered = drop_residual_toc_fragments(filtered)\n",
        "    t = \"\\n\".join(filtered)\n",
        "\n",
        "    # Extra noise cleanup\n",
        "    t = re.sub(r\"[-]{4,}\", \"\", t)\n",
        "    t = re.sub(r\"PageHeader=.*\", \"\", t)\n",
        "    t = re.sub(r\"PageNumber=.*\", \"\", t)\n",
        "    t = normalize_whitespace(t)\n",
        "\n",
        "    # Ensure big headings stay as separate lines\n",
        "    t = re.sub(r\"\\n([A-Z0-9 \\-]{4,})\\n\", r\"\\n\\1\\n\", t)\n",
        "    return t\n"
      ],
      "metadata": {
        "id": "Mf5vRYzT27og"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 5 — Canonical page mapping + final refinement\n",
        "# =====================================\n",
        "\n",
        "CANONICAL_MAP = {\n",
        "    r\"\\bhome\\s*page\\b\": \"Home Page\",\n",
        "    r\"\\bclothing\\s*page\\b\": \"Clothing Page\",\n",
        "    r\"\\border\\s*us\\s*page\\b\": \"Order Us Page\",\n",
        "    r\"\\border\\s*us\\b\": \"Order Us Page\",\n",
        "    r\"\\bcontact\\s*us\\s*page\\b\": \"Contact Us Page\",\n",
        "    r\"\\babout\\s*us\\s*page\\b\": \"About Us Page\",\n",
        "    r\"\\btrack\\s*for\\s*admin\\s*page\\b\": \"Track For Admin Page\",\n",
        "    r\"\\btrack\\b\": \"Track\",\n",
        "    r\"\\bregister\\s*page\\b\": \"Register Page\",\n",
        "    r\"\\blogin\\s*page\\b\": \"Login Page\",\n",
        "    r\"\\badmin\\s*page\\b\": \"Admin Page\",\n",
        "    r\"\\border\\s*view\\s*for\\s*user\\b\": \"Order View for User\",\n",
        "    r\"\\bpaypal\\s*for\\s*payment\\b\": \"PayPal For Payment\",\n",
        "    r\"\\bsuccess\\s*page\\b\": \"Success Page\",\n",
        "    r\"\\bfailed\\s*page\\b\": \"Failed Page\",\n",
        "}\n",
        "\n",
        "STOP_HEADINGS = {\n",
        "    \"ABSTRACT\",\n",
        "    \"ACKNOWLEDGEMENTS\",\n",
        "    \"ACKNOWLEDGEMENT\",\n",
        "    \"TABLE OF CONTENTS\",\n",
        "    \"CONTENTS\",\n",
        "    \"INDEX\",\n",
        "    \"LIST OF FIGURES\",\n",
        "    \"REFERENCES\",\n",
        "}\n",
        "\n",
        "IRRELEVANT_PATTERNS = [\n",
        "    r\"https?://\\S+\",\n",
        "    r\"opus@govst.edu\",\n",
        "    r\"^page \\d+$\",\n",
        "    r\"^\\d+\\s*$\",\n",
        "]\n",
        "\n",
        "def is_irrelevant_line(line: str) -> bool:\n",
        "    \"\"\"\n",
        "    Return True if a line is clearly not useful as content/heading.\n",
        "    \"\"\"\n",
        "    s = line.strip()\n",
        "    if not s:\n",
        "        return True  # empty lines irrelevant for our purposes\n",
        "\n",
        "    low = s.lower()\n",
        "\n",
        "    # URL / email / trivial numeric lines\n",
        "    for pat in IRRELEVANT_PATTERNS:\n",
        "        if re.search(pat, low, flags=re.IGNORECASE):\n",
        "            return True\n",
        "\n",
        "    # Very punctuation-heavy → probably junk or TOC artifact\n",
        "    punct_ratio = len(re.sub(r\"[A-Za-z0-9\\s]\", \"\", s)) / max(1, len(s))\n",
        "    if punct_ratio > 0.25:\n",
        "        return True\n",
        "\n",
        "    # Bare page numbers like \"12.\"\n",
        "    if re.match(r'^\\d{1,3}\\.?$', s):\n",
        "        return True\n",
        "\n",
        "    # Known boilerplate headings\n",
        "    if s.upper() in STOP_HEADINGS:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def normalize_heading_variant(ln: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize a heading/page-like line into a clean, title-cased label\n",
        "    using CANONICAL_MAP when possible.\n",
        "    \"\"\"\n",
        "    ln0 = ln.strip()\n",
        "\n",
        "    # Remove leading numbering like \"4.1 HOME PAGE\"\n",
        "    ln0 = re.sub(r'^\\s*\\d+(\\.\\d+)*\\s*[:\\-\\)]*\\s*', '', ln0)\n",
        "\n",
        "    # Remove trailing page numbers\n",
        "    ln0 = re.sub(r'\\s+\\d{1,3}\\s*$', '', ln0)\n",
        "\n",
        "    # Remove leading bullets\n",
        "    ln0 = re.sub(r'^[\\-\\u2022\\uf0d8\\uf0b7\\*]+\\s*', '', ln0)\n",
        "\n",
        "    # Simplify to letters/digits/spaces, and lowercase\n",
        "    key = re.sub(r'[^A-Za-z0-9 ]+', ' ', ln0).strip().lower()\n",
        "\n",
        "    # Map to canonical if pattern matches\n",
        "    for pat, canon in CANONICAL_MAP.items():\n",
        "        if re.search(pat, key, flags=re.IGNORECASE):\n",
        "            return canon\n",
        "\n",
        "    # If it already looks like \"... page\", keep but title-case\n",
        "    if key.endswith(\" page\"):\n",
        "        return \" \".join(w.capitalize() for w in key.split())\n",
        "\n",
        "    # Drop super short / pure numeric stuff\n",
        "    if len(key) < 3 or re.search(r'^\\d+$', key):\n",
        "        return \"\"\n",
        "\n",
        "    # Fallback: title-case the cleaned key\n",
        "    return \" \".join(w.capitalize() for w in key.split())\n",
        "\n",
        "def refine_cleaned_text_and_pages(cleaned_text: str) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Final step:\n",
        "      - Skip preface lines until 'ABSTRACT' or '1.0'\n",
        "      - Drop irrelevant lines\n",
        "      - (Temporary) pages will be replaced by stricter detector later\n",
        "      - Wrap trimmed text in <<<PROJECT>>> block\n",
        "    \"\"\"\n",
        "    lines = cleaned_text.splitlines()\n",
        "\n",
        "    # Start from ABSTRACT or 1.0 if present\n",
        "    start_idx = 0\n",
        "    for i, ln in enumerate(lines[:200]):\n",
        "        if re.search(r'\\babstract\\b', ln, flags=re.IGNORECASE) or re.search(r'\\b1\\.0\\b', ln):\n",
        "            start_idx = i\n",
        "            break\n",
        "\n",
        "    useful_lines = lines[start_idx:]\n",
        "    trimmed_lines = [ln for ln in useful_lines if not is_irrelevant_line(ln)]\n",
        "    trimmed_text = \"\\n\".join(trimmed_lines).strip()\n",
        "\n",
        "    # We'll recompute pages with a stricter detector in the next cell.\n",
        "    pages = []  # placeholder\n",
        "\n",
        "    project_block = \"<<<PROJECT>>>\\n\" + trimmed_text + \"\\n<<<ENDPROJECT>>>\"\n",
        "\n",
        "    return {\n",
        "        \"block\": project_block,\n",
        "        \"pages\": pages,\n",
        "        \"trimmed_text\": trimmed_text,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "TmaTlxnM3CTN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 6 — Run full pipeline and save outputs\n",
        "# =====================================\n",
        "\n",
        "# 1. Extract raw text\n",
        "raw = extract_pdf_text(PDF_PATH)\n",
        "\n",
        "# 2. Run cleaning + TOC removal\n",
        "cleaned = clean_text_pipeline_with_toc_removal(raw)\n",
        "\n",
        "# 3. Final refinement: block + trimmed text\n",
        "refined = refine_cleaned_text_and_pages(cleaned)\n",
        "\n",
        "# Save outputs in project root\n",
        "TRIMMED_TEXT_PATH = PROJECT_ROOT / \"trimmed_project_text.txt\"\n",
        "BLOCK_PATH = PROJECT_ROOT / \"full_text_of_pdf_block.txt\"\n",
        "PAGES_PATH = PROJECT_ROOT / \"detected_webpages.json\"  # will be filled next cell\n",
        "\n",
        "write_text(TRIMMED_TEXT_PATH, refined[\"trimmed_text\"])\n",
        "write_text(BLOCK_PATH, refined[\"block\"])\n",
        "# temporarily write empty list for pages; we overwrite in next cell\n",
        "write_text(PAGES_PATH, json.dumps(refined[\"pages\"], indent=2))\n",
        "\n",
        "print(\"Saved trimmed project text to:\", TRIMMED_TEXT_PATH)\n",
        "print(\"Saved project block to:\", BLOCK_PATH)\n",
        "print(\"Draft pages (to be overwritten) saved to:\", PAGES_PATH)\n",
        "\n",
        "print(\"\\n--- PROJECT BLOCK PREVIEW (first 800 chars) ---\\n\")\n",
        "print(refined[\"block\"][:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq5U5Mkh3au3",
        "outputId": "9546260d-6f7e-44d7-8451-632bb23364c3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text using PyPDF.\n",
            "Removed TOC/index block ranges: [(121, 123)]\n",
            "Saved trimmed project text to: /content/drive/MyDrive/AgileAI/Part 1/trimmed_project_text.txt\n",
            "Saved project block to: /content/drive/MyDrive/AgileAI/Part 1/full_text_of_pdf_block.txt\n",
            "Draft pages (to be overwritten) saved to: /content/drive/MyDrive/AgileAI/Part 1/detected_webpages.json\n",
            "\n",
            "--- PROJECT BLOCK PREVIEW (first 800 chars) ---\n",
            "\n",
            "<<<PROJECT>>>\n",
            "In today's fast -changing business environment, it's extremely important to be able to respond\n",
            "to client needs in the most effective and timely manner. If your customers wish to see your\n",
            "business online and have instant access to your products or services.\n",
            "Online Shopping is a lifestyle e -commerce web application, which retails various fashion and\n",
            "lifestyle products (Currently Men's Wear). This project allows viewing various products\n",
            "available enables registered users to purchase desired products instantly using PayPal payment\n",
            "processor (Instant Pay) a nd also can place order by using Cash on Delivery (Pay Later) option.\n",
            "This project provides an easy access to Administrators and Managers to view orders placed\n",
            "using Pay Later and Instant Pay options.\n",
            "In order to develop an e \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 7 — Stricter generic UI page detection\n",
        "# =====================================\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def detect_ui_pages_from_text(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Detect 'UI pages' in a generic way:\n",
        "      - Only consider lines that contain the word 'page' (word-level).\n",
        "      - Ignore irrelevant / noisy lines (using is_irrelevant_line).\n",
        "      - Require lines to be short-ish (<= 100 chars, <= 10 words).\n",
        "      - Normalize them via normalize_heading_variant (which uses CANONICAL_MAP).\n",
        "    This will pick:\n",
        "      - '4.1 HOME PAGE'\n",
        "      - '4.2 CLOTHING PAGE ( PRODUCTS )'\n",
        "      - 'Contact Us Page'\n",
        "    and ignore things like '1 Data Description', 'Use Case Diagram', etc.\n",
        "    \"\"\"\n",
        "    lines = text.splitlines()\n",
        "    candidates = []\n",
        "\n",
        "    for ln in lines:\n",
        "        if is_irrelevant_line(ln):\n",
        "            continue\n",
        "\n",
        "        s = ln.strip()\n",
        "        if not s:\n",
        "            continue\n",
        "\n",
        "        # Keep short headings; avoid full paragraphs\n",
        "        if len(s) > 100:\n",
        "            continue\n",
        "\n",
        "        # Must contain the word 'page' (word-level)\n",
        "        if not re.search(r\"\\bpage\\b\", s, flags=re.IGNORECASE):\n",
        "            continue\n",
        "\n",
        "        # Avoid long headings with too many words\n",
        "        if len(s.split()) > 10:\n",
        "            continue\n",
        "\n",
        "        norm = normalize_heading_variant(s)\n",
        "        if norm:\n",
        "            candidates.append(norm)\n",
        "\n",
        "    # Preserve order but dedupe\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    for c in candidates:\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            ordered.append(c)\n",
        "\n",
        "    return ordered\n",
        "\n",
        "# --- Recompute UI pages from the *trimmed* text and overwrite detected_webpages.json ---\n",
        "\n",
        "trimmed_text = TRIMMED_TEXT_PATH.read_text(encoding=\"utf-8\")\n",
        "ui_pages = detect_ui_pages_from_text(trimmed_text)\n",
        "\n",
        "write_text(PAGES_PATH, json.dumps(ui_pages, indent=2))\n",
        "\n",
        "print(\"\\n--- FINAL DETECTED UI PAGES ---\")\n",
        "print(json.dumps(ui_pages, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGLAGNO53dut",
        "outputId": "9ce8b819-38fa-453d-cac9-815ccad21366"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FINAL DETECTED UI PAGES ---\n",
            "[\n",
            "  \"Home Page\",\n",
            "  \"Clothing Page\",\n",
            "  \"Order Us Page\",\n",
            "  \"Contact Us Page\",\n",
            "  \"About Us Page\",\n",
            "  \"Track For Admin Page\",\n",
            "  \"Register Page\",\n",
            "  \"Admin Page\",\n",
            "  \"Success Page\",\n",
            "  \"Failed Page\",\n",
            "  \"Login Page\",\n",
            "  \"Page\"\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering"
      ],
      "metadata": {
        "id": "WeDOAqAI7ZRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Prompt Engineering - Cell 1\n",
        "# Load project block + pages, define prompt template\n",
        "# =====================================\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Reuse paths from preprocessing\n",
        "print(\"BLOCK_PATH:\", BLOCK_PATH)\n",
        "print(\"PAGES_PATH:\", PAGES_PATH)\n",
        "\n",
        "# Load the cleaned project block and canonical UI pages\n",
        "FULL_TEXT_OF_PDF = read_text(BLOCK_PATH)\n",
        "CANONICAL_PAGES = json.loads(read_text(PAGES_PATH))\n",
        "\n",
        "print(\"\\n--- DETECTED UI PAGES ---\")\n",
        "print(json.dumps(CANONICAL_PAGES, indent=2))\n",
        "\n",
        "# We'll insert this into the prompt so the model knows allowed web pages\n",
        "CANONICAL_PAGES_STR = \", \".join(CANONICAL_PAGES)\n",
        "\n",
        "# Separator between instructions+input and JSON output (used in training)\n",
        "SEPARATOR = \"\\n\\n### OUTPUT JSON START ###\\n\\n\"\n",
        "\n",
        "# ---- Prompt template ----\n",
        "# We keep it generic but structured like your original:\n",
        "PROMPT_TEMPLATE = f\"\"\"\n",
        "You are AgileAI, an assistant that helps project managers break down software projects\n",
        "into an Agile backlog structure of EPIC -> FEATURES -> USER STORIES.\n",
        "\n",
        "You will be given a SOFTWARE PROJECT DESCRIPTION, delimited by the markers:\n",
        "\n",
        "    <<<PROJECT>>>\n",
        "    (project text here)\n",
        "    <<<ENDPROJECT>>>\n",
        "\n",
        "Your job is to:\n",
        "  1. Understand the project scope, goals, and main flows.\n",
        "  2. Create ONE high-level EPIC that captures the overall project.\n",
        "  3. Create several FEATURES that group related functionality under that epic.\n",
        "  4. For each feature, create USER STORIES that are small, sprint-sized, and testable.\n",
        "  5. Return everything as a SINGLE JSON OBJECT that follows the schema below.\n",
        "\n",
        "------------------ JSON OUTPUT SCHEMA ------------------\n",
        "\n",
        "The output must be ONLY valid JSON, with this structure:\n",
        "\n",
        "{{\n",
        "  \"epic\": {{\n",
        "    \"title\": \"...\",\n",
        "    \"summary\": \"...\"\n",
        "  }},\n",
        "  \"features\": [\n",
        "    {{\n",
        "      \"title\": \"...\",\n",
        "      \"description\": \"...\",\n",
        "      \"webpage\": \"One of the allowed page names\",\n",
        "      \"stories\": [\n",
        "        {{\n",
        "          \"id\": \"F1-S1\",\n",
        "          \"title\": \"...\",\n",
        "          \"description\": \"...\",\n",
        "          \"acceptance_criteria\": [\n",
        "            \"...\",\n",
        "            \"...\",\n",
        "            ...\n",
        "          ],\n",
        "          \"definition_of_ready\": [\n",
        "            \"...\",\n",
        "            \"...\",\n",
        "            ...\n",
        "          ]\n",
        "        }}\n",
        "      ]\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Rules and constraints:\n",
        "\n",
        "- \"epic\":\n",
        "  - \"title\": short (max 10 words), high-level.\n",
        "  - \"summary\": 2–4 sentences summarizing the entire project.\n",
        "\n",
        "- \"features\":\n",
        "  - Group related functionality; 3–8 features is typical.\n",
        "  - \"title\": short (max 8 words), describing the feature.\n",
        "  - \"description\": 1–3 sentences about this feature.\n",
        "  - \"webpage\":\n",
        "      * MUST be chosen from this allowed list:\n",
        "        [{CANONICAL_PAGES_STR}]\n",
        "      * If multiple pages could apply, choose the one that best fits.\n",
        "  - \"stories\": list of user stories for that feature.\n",
        "\n",
        "- \"stories\" (user stories):\n",
        "  - Each story represents one sprint-sized chunk of work.\n",
        "  - \"id\": must be in the form \"F{{feature_index}}-S{{story_index}}\",\n",
        "      e.g., \"F1-S1\", \"F1-S2\", \"F2-S1\", etc.\n",
        "  - \"title\": short (max 10 words), in the style of a user story title.\n",
        "  - \"description\": 1–3 sentences, clearly describing the behaviour and value.\n",
        "  - \"acceptance_criteria\":\n",
        "      * 2–5 bullet points.\n",
        "      * Each item is a clear, testable condition.\n",
        "  - \"definition_of_ready\":\n",
        "      * 1–5 items describing preconditions (e.g., \"UI mockups approved\").\n",
        "\n",
        "Source-of-truth and assumptions:\n",
        "\n",
        "- Use ONLY information from the project description between <<<PROJECT>>> and <<<ENDPROJECT>>>.\n",
        "- Do NOT invent new modules or flows that are not implied by the text.\n",
        "- If you must introduce something that is not explicitly in the project text,\n",
        "  mark it clearly with the token \"[ASSUMPTION]\" inside the string.\n",
        "\n",
        "Formatting:\n",
        "\n",
        "- Output must be a single JSON object, no markdown, no comments.\n",
        "- Do NOT include the project description in the output.\n",
        "- Do NOT include explanation text; ONLY the JSON object.\n",
        "\n",
        "------------------ PROJECT DESCRIPTION ------------------\n",
        "\n",
        "{FULL_TEXT_OF_PDF}\n",
        "\n",
        "---------------------------------------------------------\n",
        "Remember:\n",
        "- Think through the project first.\n",
        "- Then produce the JSON object in the required schema.\n",
        "\"\"\"\n",
        "\n",
        "# Save a concrete prompt instance so you can inspect it\n",
        "FINAL_PROMPT_PATH = PROJECT_ROOT / \"final_prompt_input.txt\"\n",
        "write_text(FINAL_PROMPT_PATH, PROMPT_TEMPLATE + SEPARATOR)\n",
        "\n",
        "print(\"\\nSaved full prompt (instructions + project + separator) to:\")\n",
        "print(FINAL_PROMPT_PATH)\n",
        "\n",
        "print(\"\\n--- PROMPT PREVIEW (first 800 chars) ---\\n\")\n",
        "print((PROMPT_TEMPLATE + SEPARATOR)[:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XGj697i7dVC",
        "outputId": "d6a51f97-ee47-4d68-da22-ffe600ca8f61"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLOCK_PATH: /content/drive/MyDrive/AgileAI/Part 1/full_text_of_pdf_block.txt\n",
            "PAGES_PATH: /content/drive/MyDrive/AgileAI/Part 1/detected_webpages.json\n",
            "\n",
            "--- DETECTED UI PAGES ---\n",
            "[\n",
            "  \"Home Page\",\n",
            "  \"Clothing Page\",\n",
            "  \"Order Us Page\",\n",
            "  \"Contact Us Page\",\n",
            "  \"About Us Page\",\n",
            "  \"Track For Admin Page\",\n",
            "  \"Register Page\",\n",
            "  \"Admin Page\",\n",
            "  \"Success Page\",\n",
            "  \"Failed Page\",\n",
            "  \"Login Page\",\n",
            "  \"Page\"\n",
            "]\n",
            "\n",
            "Saved full prompt (instructions + project + separator) to:\n",
            "/content/drive/MyDrive/AgileAI/Part 1/final_prompt_input.txt\n",
            "\n",
            "--- PROMPT PREVIEW (first 800 chars) ---\n",
            "\n",
            "\n",
            "You are AgileAI, an assistant that helps project managers break down software projects\n",
            "into an Agile backlog structure of EPIC -> FEATURES -> USER STORIES.\n",
            "\n",
            "You will be given a SOFTWARE PROJECT DESCRIPTION, delimited by the markers:\n",
            "\n",
            "    <<<PROJECT>>>\n",
            "    (project text here)\n",
            "    <<<ENDPROJECT>>>\n",
            "\n",
            "Your job is to:\n",
            "  1. Understand the project scope, goals, and main flows.\n",
            "  2. Create ONE high-level EPIC that captures the overall project.\n",
            "  3. Create several FEATURES that group related functionality under that epic.\n",
            "  4. For each feature, create USER STORIES that are small, sprint-sized, and testable.\n",
            "  5. Return everything as a SINGLE JSON OBJECT that follows the schema below.\n",
            "\n",
            "------------------ JSON OUTPUT SCHEMA ------------------\n",
            "\n",
            "The output must be ONLY valid JSON, with this structure:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Prompt Engineering - Cell 2\n",
        "# JSON schema validator and repair helpers\n",
        "# =====================================\n",
        "\n",
        "from typing import Any, Dict, List, Tuple\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def validate_epic(epic: Dict[str, Any]) -> List[str]:\n",
        "    errs = []\n",
        "    if not isinstance(epic, dict):\n",
        "        return [\"epic must be an object\"]\n",
        "    if \"title\" not in epic or not isinstance(epic[\"title\"], str) or not epic[\"title\"].strip():\n",
        "        errs.append(\"epic.title must be a non-empty string\")\n",
        "    if \"summary\" not in epic or not isinstance(epic[\"summary\"], str) or not epic[\"summary\"].strip():\n",
        "        errs.append(\"epic.summary must be a non-empty string\")\n",
        "    return errs\n",
        "\n",
        "def validate_story(story: Dict[str, Any], feature_index: int, story_index: int) -> List[str]:\n",
        "    errs = []\n",
        "    prefix = f\"features[{feature_index}].stories[{story_index}]\"\n",
        "\n",
        "    if not isinstance(story, dict):\n",
        "        return [f\"{prefix} must be an object\"]\n",
        "\n",
        "    # id\n",
        "    sid = story.get(\"id\")\n",
        "    if not isinstance(sid, str) or not sid.strip():\n",
        "        errs.append(f\"{prefix}.id must be a non-empty string\")\n",
        "\n",
        "    # title\n",
        "    title = story.get(\"title\")\n",
        "    if not isinstance(title, str) or not title.strip():\n",
        "        errs.append(f\"{prefix}.title must be a non-empty string\")\n",
        "\n",
        "    # description\n",
        "    desc = story.get(\"description\")\n",
        "    if not isinstance(desc, str) or not desc.strip():\n",
        "        errs.append(f\"{prefix}.description must be a non-empty string\")\n",
        "\n",
        "    # acceptance_criteria\n",
        "    ac = story.get(\"acceptance_criteria\")\n",
        "    if not isinstance(ac, list) or not ac:\n",
        "        errs.append(f\"{prefix}.acceptance_criteria must be a non-empty list\")\n",
        "    else:\n",
        "        for i, item in enumerate(ac):\n",
        "            if not isinstance(item, str) or not item.strip():\n",
        "                errs.append(f\"{prefix}.acceptance_criteria[{i}] must be a non-empty string\")\n",
        "\n",
        "    # definition_of_ready\n",
        "    dor = story.get(\"definition_of_ready\")\n",
        "    if not isinstance(dor, list) or not dor:\n",
        "        errs.append(f\"{prefix}.definition_of_ready must be a non-empty list\")\n",
        "    else:\n",
        "        for i, item in enumerate(dor):\n",
        "            if not isinstance(item, str) or not item.strip():\n",
        "                errs.append(f\"{prefix}.definition_of_ready[{i}] must be a non-empty string\")\n",
        "\n",
        "    return errs\n",
        "\n",
        "def validate_feature(feature: Dict[str, Any], idx: int, allowed_pages: List[str]) -> List[str]:\n",
        "    errs = []\n",
        "    prefix = f\"features[{idx}]\"\n",
        "\n",
        "    if not isinstance(feature, dict):\n",
        "        return [f\"{prefix} must be an object\"]\n",
        "\n",
        "    # title\n",
        "    if \"title\" not in feature or not isinstance(feature[\"title\"], str) or not feature[\"title\"].strip():\n",
        "        errs.append(f\"{prefix}.title must be a non-empty string\")\n",
        "\n",
        "    # description\n",
        "    if \"description\" not in feature or not isinstance(feature[\"description\"], str) or not feature[\"description\"].strip():\n",
        "        errs.append(f\"{prefix}.description must be a non-empty string\")\n",
        "\n",
        "    # webpage\n",
        "    wp = feature.get(\"webpage\")\n",
        "    if not isinstance(wp, str) or not wp.strip():\n",
        "        errs.append(f\"{prefix}.webpage must be a non-empty string\")\n",
        "    elif allowed_pages and wp not in allowed_pages:\n",
        "        errs.append(f\"{prefix}.webpage '{wp}' is not in allowed pages {allowed_pages}\")\n",
        "\n",
        "    # stories\n",
        "    stories = feature.get(\"stories\")\n",
        "    if not isinstance(stories, list) or not stories:\n",
        "        errs.append(f\"{prefix}.stories must be a non-empty list\")\n",
        "    else:\n",
        "        for s_idx, story in enumerate(stories):\n",
        "            errs.extend(validate_story(story, idx, s_idx))\n",
        "\n",
        "    return errs\n",
        "\n",
        "def validate_schema(obj: Dict[str, Any], allowed_pages: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Top-level schema validator for the JSON structure.\n",
        "    Returns a list of error strings (empty means 'valid').\n",
        "    \"\"\"\n",
        "    errs = []\n",
        "\n",
        "    if not isinstance(obj, dict):\n",
        "        return [\"Top-level JSON must be an object\"]\n",
        "\n",
        "    # epic\n",
        "    if \"epic\" not in obj:\n",
        "        errs.append(\"Missing 'epic' object\")\n",
        "    else:\n",
        "        errs.extend(validate_epic(obj[\"epic\"]))\n",
        "\n",
        "    # features\n",
        "    features = obj.get(\"features\")\n",
        "    if not isinstance(features, list) or not features:\n",
        "        errs.append(\"'features' must be a non-empty list\")\n",
        "    else:\n",
        "        for i, feat in enumerate(features):\n",
        "            errs.extend(validate_feature(feat, i, allowed_pages))\n",
        "\n",
        "    return errs\n",
        "\n",
        "\n",
        "# ---------- Repair helpers ----------\n",
        "\n",
        "def fuzzy_match_page_name(name: str, allowed_pages: List[str], threshold: float = 0.6) -> str:\n",
        "    \"\"\"\n",
        "    If 'name' is not exactly in allowed_pages, try to fuzzy-match it.\n",
        "    Returns a best match or the original name if nothing is close enough.\n",
        "    \"\"\"\n",
        "    if not allowed_pages:\n",
        "        return name\n",
        "    best_score = 0.0\n",
        "    best_page = name\n",
        "    for p in allowed_pages:\n",
        "        score = SequenceMatcher(None, name.lower(), p.lower()).ratio()\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_page = p\n",
        "    if best_score >= threshold:\n",
        "        return best_page\n",
        "    return name\n",
        "\n",
        "def repair_story_ids(features: List[Dict[str, Any]]) -> None:\n",
        "    \"\"\"\n",
        "    Ensure story 'id' fields follow F{feature_index}-S{story_index}.\n",
        "    Modifies the features list in-place.\n",
        "    \"\"\"\n",
        "    for f_idx, feat in enumerate(features, start=1):\n",
        "        stories = feat.get(\"stories\", [])\n",
        "        if not isinstance(stories, list):\n",
        "            continue\n",
        "        for s_idx, story in enumerate(stories, start=1):\n",
        "            if not isinstance(story, dict):\n",
        "                continue\n",
        "            story[\"id\"] = f\"F{f_idx}-S{s_idx}\"\n",
        "\n",
        "def repair_webpages(features: List[Dict[str, Any]], allowed_pages: List[str]) -> None:\n",
        "    \"\"\"\n",
        "    Fuzzy-match feature.webpage to the closest allowed page.\n",
        "    Modifies the features list in-place.\n",
        "    \"\"\"\n",
        "    if not allowed_pages:\n",
        "        return\n",
        "    for feat in features:\n",
        "        if not isinstance(feat, dict):\n",
        "            continue\n",
        "        wp = feat.get(\"webpage\")\n",
        "        if isinstance(wp, str) and wp.strip():\n",
        "            feat[\"webpage\"] = fuzzy_match_page_name(wp, allowed_pages)\n",
        "\n",
        "def repair_json(obj: Dict[str, Any], allowed_pages: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Apply simple, conservative repairs:\n",
        "      - Fix story IDs to consistent pattern.\n",
        "      - Fuzzy-match feature.webpage names to allowed_pages.\n",
        "    Returns the modified object.\n",
        "    \"\"\"\n",
        "    if not isinstance(obj, dict):\n",
        "        return obj\n",
        "\n",
        "    features = obj.get(\"features\")\n",
        "    if isinstance(features, list):\n",
        "        repair_story_ids(features)\n",
        "        repair_webpages(features, allowed_pages)\n",
        "\n",
        "    return obj\n",
        "\n",
        "\n",
        "def validate_and_repair(obj: Dict[str, Any], allowed_pages: List[str]) -> Tuple[Dict[str, Any], List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Run validation, then attempt repairs, then re-validate.\n",
        "\n",
        "    Returns:\n",
        "      - repaired_obj\n",
        "      - errors_before\n",
        "      - errors_after\n",
        "    \"\"\"\n",
        "    errs_before = validate_schema(obj, allowed_pages)\n",
        "    if not errs_before:\n",
        "        # Already valid\n",
        "        return obj, [], []\n",
        "\n",
        "    repaired = repair_json(obj, allowed_pages)\n",
        "    errs_after = validate_schema(repaired, allowed_pages)\n",
        "    return repaired, errs_before, errs_after\n",
        "\n",
        "print(\"Validator and repair helpers defined. They will be used after generation.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGJXdyY17eO5",
        "outputId": "9454025d-2277-4017-cb25-7ef61f79a998"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validator and repair helpers defined. They will be used after generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "PM6lQ0XO7ufp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"transformers>=4.39.0\" \"datasets>=2.18.0\" \"accelerate>=0.30.0\" \\\n",
        "               \"bitsandbytes>=0.43.0\" \"peft>=0.10.0\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEqYFTY87yW4",
        "outputId": "38f98fa9-a044-4aca-cf53-7a334a60fae4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# --- Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# TinyLlama tokenizer sometimes has no pad_token → reuse eos_token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 4-bit quantization config (QLoRA style) ---\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# --- Load base model in 4-bit ---\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # spreads across GPU(s) if available\n",
        ")\n",
        "\n",
        "# Ensure embedding size matches tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Prepare for k-bit training (freezes norms, etc.)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# --- LoRA config (only trains a few adapter weights) ---\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "8Om-nLgQB6Fw",
        "outputId": "8ee71502-ead7-4d76-e839-af8b52fe3f9f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4264594341.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# --- Load base model in 4-bit ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4879\u001b[0m                 )\n\u001b[1;32m   4880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4881\u001b[0;31m         hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n\u001b[0m\u001b[1;32m   4882\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4883\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py\u001b[0m in \u001b[0;36mget_hf_quantizer\u001b[0;34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_library_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mc5f2ulBB8Y2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "didb7BZsCHul"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yiubI9zlCUIQ"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}